{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ordered-scroll",
   "metadata": {},
   "source": [
    "# Machine Learning for Healthcare (Interpretability)\n",
    "This Jupyter notebook provides some demonstrations of how one could analyze the Bilirubin data set. These include:\n",
    "* basic data analysis:\n",
    "    * determine data set size\n",
    "    * determine balance of data (is it balanced or imbalanced)\n",
    "    * investigate variability of data using Principal Component Analysis (PCA)\n",
    "* Using the Logistic Regression (LR) classifier to analyze Bilirubin data\n",
    "* Using the Random Forest (RF) classifier to analyze Bilirubin data\n",
    "    * Comparing the impact of varying forest and tree sizes\n",
    "    * Comparing to the LR classifier\n",
    "* Feature selection: Using RF and LR to select the most important features from the dataset\n",
    "* Dataset balance: Use downsampling and upsampling to balance the dataset and compare performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-sixth",
   "metadata": {},
   "source": [
    "## Reading in raw data from CSV\n",
    "The raw data for this exercise is stored in a CSV file. The first 10 rows of the data are provided below, just to give you an idea of what it looks like:\n",
    "\n",
    "```\n",
    "hours_since_birth,GA (days),BiliBGA,Weight,Bili_Weight_ratio,MothersAge,IsPreterm,Arterial_pH,hasFTlimit\n",
    "7.58105,243,118,1929,0.061304,30,1,7.42,1\n",
    "62.65124,264,8,2577,0.003085,34,0,6.22,0\n",
    "30.56771,274,94,3302,0.028395,38,0,7.35,0\n",
    "60.80173,260,66,2183,0.030425,39,0,7.21,0\n",
    "77.64267,228,46,2568,0.017848,37,1,7.13,0\n",
    "35.92315,233,70,1964,0.035554,28,1,7.14,0\n",
    "110.58025,264,90,2972,0.030240,29,0,6.28,1\n",
    "105.23609,263,117,2548,0.045773,34,0,6.93,0\n",
    "1.90231,291,86,4056,0.021273,39,0,7.47,0\n",
    "36.82003,230,99,2073,0.047600,30,1,7.41,0\n",
    "\n",
    "```\n",
    "\n",
    "We'll be using the [Pandas](https://pandas.pydata.org/) library to read in and work with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_frame = pd.read_csv(\"/cluster/courses/ml4h/data_for_users/data/bili_generated_ext.csv\")\n",
    "data_frame = data_frame[0:2000]  # Only use the first 2000 samples in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-creation",
   "metadata": {},
   "source": [
    "Let's take another look at the data, in a nicer format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-agency",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_frame[0:10].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-riverside",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "Now, let's look at the data a little bit closer. The `shape` attribute of the DataFrame will tell us how many rows and columns there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_frame.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-defeat",
   "metadata": {},
   "source": [
    "Next, let's dig a little deeper into the composition of the dataset. In our case, the `hasFTlimit` column denotes whether or not the baby requires phototherapy. How many require it and how many do not? We can determine this by comparing the `hasFTlimit` column to either `True` or `False`, and counting the number of matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the target column from the data set, it should not be part of the input features\n",
    "target_column = data_frame.pop(\"hasFTlimit\")\n",
    "count_requires_treatment = len(target_column[target_column == True])\n",
    "no_treatment_required = len(target_column[target_column == False])\n",
    "print(\"Requires treatment: %i\" % count_requires_treatment)\n",
    "print(\"No treatment required: %i\" % no_treatment_required)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-share",
   "metadata": {},
   "source": [
    "Thus, we can see that the dataset is very imbalanced. This will be explored later. For now, let's continue by taking a look at the variability present in the data using [Principal Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis).\n",
    "\n",
    "We will define a function that performs PCA for a given input dataset, since we will repeat this analysis several more times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def do_pca(dataset, target_column):\n",
    "    # Declare and fit PCA object, transform data\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(dataset)\n",
    "    pca_output = pca.transform(dataset)\n",
    "\n",
    "    # Plot transformed data\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    sns.scatterplot(x=pca_output[:,0], y=pca_output[:,1], hue=target_column, alpha=0.7)\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data (remove mean and scale to unit variance)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standardized_data = StandardScaler().fit_transform(data_frame)\n",
    "\n",
    "# Call the function\n",
    "do_pca(standardized_data, target_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-examination",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Now we will fit a logistic regression classifier to the standardized data, and then use it to classify the same data, using simple accuracy as a performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define function to fit and return LR classifier\n",
    "def fit_lr_classifier(dataset, labels):\n",
    "    # Fit classifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    lr_classifier = LogisticRegression(solver=\"lbfgs\", max_iter=200)\n",
    "    lr_classifier.fit(dataset, labels)\n",
    "    return lr_classifier\n",
    "\n",
    "# Get classifier\n",
    "lr_classifier = fit_lr_classifier(standardized_data, target_column)\n",
    "\n",
    "# Get predictions\n",
    "predictions = lr_classifier.predict_proba(standardized_data)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.sum(predicted_labels == target_column) / len(target_column)\n",
    "print(\"Accuracy: %f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-clause",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "Accuracy is not the only way (or perhaps even the best way) to evaluate the performance of a classifier. Let's additionally calculate the AUROC and AUPRC metrics, and plot the respective curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to evaluate the performance of a classifier based on the results\n",
    "def evaluate_classifier(results):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.metrics import roc_auc_score, roc_curve, average_precision_score, precision_recall_curve\n",
    "    \n",
    "    # Set up AUROC plot\n",
    "    figure = plt.figure(figsize=(10, 8))\n",
    "    ax = sns.lineplot(x=[0, 1], y=[0, 1], color=\"red\", label=\"Random guess area 0.5\")  # Plot random guess threshold\n",
    "    \n",
    "    # Iterate over provided result sets\n",
    "    for data_name, data_value in results.items():\n",
    "        # Unpack values\n",
    "        predictions, target_column = data_value\n",
    "        \n",
    "        # Calculate the AUROC score\n",
    "        auroc_score = roc_auc_score(target_column, predictions[:, 1])\n",
    "\n",
    "        # Plot the receiver operating characteristic curve\n",
    "        roc_fpr, roc_tpr, _ = roc_curve(target_column, predictions[:, 1])\n",
    "\n",
    "        ax = sns.lineplot(x=roc_fpr, y=roc_tpr, label=\"%s area %f\" % (data_name, auroc_score))\n",
    "\n",
    "    # Set options and show plot\n",
    "    ax.set_title(\"Receiver Operating Characteristic\")\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Set up precision recall plot\n",
    "    figure = plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Calculate PR random guess baseline as len(positive samples) / len(total samples)\n",
    "    pr_baseline = np.sum(results[list(results.keys())[0]][1]) / len(results[list(results.keys())[0]][1])\n",
    "    ax = sns.lineplot(x=[0, 1], y=[pr_baseline, pr_baseline], color=\"red\", label=\"Random guess area %f\" % pr_baseline)\n",
    "    \n",
    "    # Iterate over provided result sets\n",
    "    for data_name, data_value in results.items():\n",
    "        # Unpack values\n",
    "        predictions, target_column = data_value\n",
    "    \n",
    "        # Calculate average precision\n",
    "        avg_precision = average_precision_score(target_column, predictions[:, 1])\n",
    "    \n",
    "        # Plot the precision recall curve\n",
    "        precision, recall, _ = precision_recall_curve(target_column, predictions[:, 1])\n",
    "        ax = sns.lineplot(x=recall, y=precision, label=\"%s area %f\" % (data_name, avg_precision))\n",
    "    \n",
    "    # Set options and show plot\n",
    "    ax.set_title(\"Precision Recall\")\n",
    "    ax.set_xlabel(\"Recall\")\n",
    "    ax.set_ylabel(\"Precision\")\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc=\"lower left\")\n",
    "    plt.show()\n",
    "\n",
    "# Do evaluation\n",
    "evaluate_classifier({'Bilirubin dataset': (predictions, target_column)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-restoration",
   "metadata": {},
   "source": [
    "Now we will scale the dataset (as before), and split the dataset into a training and testing sets, using a 70%/30% split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-checkout",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(data_frame)\n",
    "standardized_data = scaler.transform(data_frame)\n",
    "\n",
    "# Train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(standardized_data, target_column, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-burner",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-punishment",
   "metadata": {},
   "source": [
    "Next, we will train a [RF classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) using 100 trees, each with a max depth of 3, and 5 fold cross validation. We will also define a set of metrics to use for evaluation (the same metrics as in the last notebook), and use them to obtain a performance baseline for this classifier. Of the 5 folds, we select the classifier with the best performance in AUROC as the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Evaluation metrics\n",
    "eval_metrics = {'AUROC': 'roc_auc', 'avg_precision': 'average_precision', 'Accuracy': make_scorer(accuracy_score)}\n",
    "\n",
    "# Train with cross validation\n",
    "cv_results = cross_validate(RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42), x_train, y_train, cv=5, scoring=eval_metrics, return_estimator=True)\n",
    "\n",
    "# Determine the best classifier from the folds, and use it as the baseline\n",
    "best_idx = np.argmax(cv_results['test_AUROC'])\n",
    "baseline_clf = cv_results['estimator'][best_idx]\n",
    "\n",
    "print(\"Baseline performance:\")\n",
    "print(\"AUROC: %f, Avg. Precision: %f, Accuracy: %f\" % (cv_results['test_AUROC'][best_idx], cv_results['test_avg_precision'][best_idx], cv_results['test_Accuracy'][best_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-darwin",
   "metadata": {},
   "source": [
    "Let's now take a closer look at how the hyperparameters of the classifier impact the performance. In this example, the two hyperparameters we'll adjust are the number of trees in the random forest, and the depth of the trees.  We are interested in finding the best combination of these parameters -- this is a classic example of a [grid-search](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search), and we will use the [corresponding functionality of the scikit learn library](https://scikit-learn.org/stable/modules/grid_search.html#multimetric-grid-search) to help exceute it. Note that in this cell we will use 3 fold CV instead of 5 fold CV in the interest of saving computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {'n_estimators': [50, 100, 150], 'max_depth': [1, 3, 5]}\n",
    "\n",
    "# Execute grid search\n",
    "gs = GridSearchCV(RandomForestClassifier(random_state=42), param_grid=param_grid, scoring=eval_metrics, refit='AUROC', cv=3)\n",
    "gs.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-yemen",
   "metadata": {},
   "source": [
    "Given that we defined a 3x3 grid of parameter values to search, we expect to have results for 9 different permutations of parameter values. Indeed, we can see the permuatations tested in the grid search, as well as their results. Given that we have been using *AUROC* as our primary metric, which model parameters do you think performs best? How do the results vary with different parameter values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Permutations:\")\n",
    "print(gs.cv_results_['params'])\n",
    "\n",
    "print(\"Mean AUROC result across folds:\")\n",
    "print(gs.cv_results_['mean_test_AUROC'])\n",
    "\n",
    "print(\"Mean avg. precision result across folds:\")\n",
    "print(gs.cv_results_['mean_test_avg_precision'])\n",
    "\n",
    "print(\"Mean accuracy result across folds:\")\n",
    "print(gs.cv_results_['mean_test_Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-transition",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: \" + str(gs.best_params_))\n",
    "grid_search_clf = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-therapy",
   "metadata": {},
   "source": [
    "Let's now compare the performance of our baseline classifier and the best classifier from our grid search on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute evaluation metrics for trained classifiers\n",
    "def evaluate_trained_clf(clf, x_test, y_test, scoring):\n",
    "    from sklearn.metrics import get_scorer\n",
    "    for score, score_fn in scoring.items():\n",
    "        # Fix up function reference\n",
    "        score_fn = get_scorer(score_fn) if type(score_fn) == str else score_fn\n",
    "        print(\"%s: %f\" % (score, score_fn(clf, x_test, y_test)))\n",
    "\n",
    "print(\"Baseline classifier:\")\n",
    "evaluate_trained_clf(baseline_clf, x_test, y_test, eval_metrics)\n",
    "print()\n",
    "\n",
    "print(\"Grid search classifier:\")\n",
    "evaluate_trained_clf(grid_search_clf, x_test, y_test, eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-journalism",
   "metadata": {},
   "source": [
    "Let's also compare to the LR classifier, this time using the LASSO (L1 regularization) penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to fit and return LR classifier\n",
    "def fit_lr_classifier(dataset, labels, C=1.0):\n",
    "    # Fit classifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    lr_classifier = LogisticRegression(solver=\"liblinear\", penalty=\"l1\", C=C, max_iter=200)\n",
    "    lr_classifier.fit(dataset, labels)\n",
    "    return lr_classifier\n",
    "\n",
    "lr_classifier = fit_lr_classifier(x_train, y_train, 0.1)\n",
    "print(\"LR classifier:\")\n",
    "evaluate_trained_clf(lr_classifier, x_test, y_test, eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-paraguay",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "The random forest classifier exposes the feature importances as determined during training in the `feature_importances_` parameter. Let's see how this looks for our dataset. Which features do you think matter more for prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-winter",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, imp in enumerate(grid_search_clf.feature_importances_):\n",
    "    print(\"%s: %f\" % (data_frame.columns[idx], imp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-right",
   "metadata": {},
   "source": [
    "The logistic regression classifier doesn't expose feature importances directly, however the sign and magnitudes of the learned feature coefficients can be used to judge the influence of each feature on the overall prediction: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-static",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, imp in enumerate(lr_classifier.coef_[0]):\n",
    "    print(\"%s: %f\" % (data_frame.columns[idx], imp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-bryan",
   "metadata": {},
   "source": [
    "As it is a little difficult to interpret these values simply by looking at them, let's try a little visualization. One typical way to compare the coefficients or importance learned by these classifiers is to plot a barchart. Let's do this for both the LR and RF classifiers. Was the LASSO penalty able to induce some sparsity in the LR classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "sns.barplot(x=data_frame.columns, y=lr_classifier.coef_[0])\n",
    "ax.set_title(\"Logistic Regression w/ LASSO coefficients\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "sns.barplot(x=data_frame.columns, y=grid_search_clf.feature_importances_)\n",
    "ax.set_title(\"Random Forest feature importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-myanmar",
   "metadata": {},
   "source": [
    "Exercise: Update the `C` value in the cell below, rerunning the cell as necessary, and try to induce more sparsity (hint: the C parameter is the *inverse* of the regularization strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 0.1\n",
    "new_lr_classifier = fit_lr_classifier(x_train, y_train, C)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "sns.barplot(x=data_frame.columns, y=new_lr_classifier.coef_[0])\n",
    "ax.set_title(\"Logistic Regression w/ LASSO coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-above",
   "metadata": {},
   "source": [
    "Another visualization that might give some insight into feature importance is the [partial dependence plot](https://scikit-learn.org/stable/modules/partial_dependence.html). These plots shed some light onto how each feature impacts the final classification result. Since we have scaled the input data, we need to add some extra code if we want the values on the axes of the charts to correspond to the original values. Here, we will create partial dependence plots for two input features (Bili_weight_ratio and MothersAge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for restoring original data values\n",
    "def inverse_transform_dim(scaler, dim, val):\n",
    "    return \"%.2f\" % (scaler.var_[dim] ** 0.5 * val + scaler.mean_[dim])\n",
    "\n",
    "def transform_x_labels(axes, dim, scaler):\n",
    "    orig_x_values = axes.get_xticks()\n",
    "    transformed_x_values = [inverse_transform_dim(scaler, dim, x) for x in orig_x_values]\n",
    "    axes.set_xticks(orig_x_values)\n",
    "    axes.set_xticklabels(transformed_x_values)\n",
    "    axes.set_xlim(orig_x_values[1], orig_x_values[-2])\n",
    "\n",
    "def transform_y_labels(axes, dim, scaler):\n",
    "    orig_y_values = axes.get_yticks()\n",
    "    transformed_y_values = [inverse_transform_dim(scaler, dim, y) for y in orig_y_values]\n",
    "    axes.set_yticks(orig_y_values)\n",
    "    axes.set_yticklabels(transformed_y_values)\n",
    "    axes.set_ylim(orig_y_values[1], orig_y_values[-2])\n",
    "\n",
    "# Plot partial dependence charts\n",
    "charts_to_create = [4, 5]\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "dpd = plot_partial_dependence(grid_search_clf, x_test, charts_to_create, feature_names=data_frame.columns, ax=ax)\n",
    "\n",
    "# Transform labels on x-axis\n",
    "k = 0\n",
    "for i in range(len(dpd.axes_)):\n",
    "    for j in range(len(dpd.axes_[i])):\n",
    "        if dpd.axes_[i][j] is None:\n",
    "            continue\n",
    "            \n",
    "        transform_x_labels(dpd.axes_[i][j], charts_to_create[k], scaler)\n",
    "        k += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-switch",
   "metadata": {},
   "source": [
    "Note that it is also possible to combine two features and create a partial dependence plot that shows how these to features jointly impact the outcome. To do this, simply pass a tuple instead of a feature ID to the plot_partial_dependence function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-prior",
   "metadata": {},
   "source": [
    "## Dataset Balance\n",
    "Earlier, we saw how the Bilirubin dataset is unbalanced. Let's balance the dataset using both down and upsampling, and see how this impacts the performance. To do this, we will use the [imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn) library to help us out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "print(\"Original sample count: %i\" % len(target_column))\n",
    "print(\"Positive class sample count: %i\"  % len(target_column[target_column == True]))\n",
    "print(\"Negative class sample count: %i\"  % len(target_column[target_column == False]))\n",
    "print()\n",
    "\n",
    "# Create new balanced dataset using downsampling\n",
    "downsampled_x, downsampled_y = RandomUnderSampler().fit_resample(standardized_data, target_column)\n",
    "print(\"Downsampled sample count: %i\" % len(downsampled_y))\n",
    "print(\"Downsampled positive class sample count: %i\"  % len(downsampled_y[downsampled_y == True]))\n",
    "print(\"Negative class sample count: %i\"  % len(downsampled_y[downsampled_y == False]))\n",
    "print()\n",
    "\n",
    "# Create new balanced dataset using upsampling\n",
    "upsampled_x, upsampled_y = RandomOverSampler().fit_resample(standardized_data, target_column)\n",
    "print(\"Upsampled sample count: %i\" % len(upsampled_y))\n",
    "print(\"Upsampled positive class sample count: %i\"  % len(upsampled_y[upsampled_y == True]))\n",
    "print(\"Negative class sample count: %i\"  % len(upsampled_y[upsampled_y == False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-device",
   "metadata": {},
   "source": [
    "Now let's evaluate the training performance of the down and up sampled classifiers with 5 fold CV. How does the difference in performance between the balanced and unbalanced RF classifiers compare? Which metrics are impacted, and which are not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train downsampled classifier with 5 fold CV\n",
    "down_x_train, down_x_test, down_y_train, down_y_test = train_test_split(downsampled_x, downsampled_y, test_size=0.3)\n",
    "down_cv_results = cross_validate(RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42), down_x_train, down_y_train, cv=5, scoring=eval_metrics, return_estimator=True)\n",
    "down_best_idx = np.argmax(down_cv_results['test_AUROC'])\n",
    "down_clf = down_cv_results['estimator'][down_best_idx]\n",
    "\n",
    "print(\"Unbalanced training performance:\")\n",
    "print(\"AUROC: %f, Avg. Precision: %f, Accuracy: %f\" % (cv_results['test_AUROC'][best_idx], cv_results['test_avg_precision'][best_idx], cv_results['test_Accuracy'][best_idx]))\n",
    "print()\n",
    "\n",
    "print(\"Downsampled training performance:\")\n",
    "print(\"AUROC: %f, Avg. Precision: %f, Accuracy: %f\" % (down_cv_results['test_AUROC'][down_best_idx], down_cv_results['test_avg_precision'][down_best_idx], down_cv_results['test_Accuracy'][down_best_idx]))\n",
    "print()\n",
    "\n",
    "# Train upsampled classifier with 5 fold CV\n",
    "up_x_train, up_x_test, up_y_train, up_y_test = train_test_split(upsampled_x, upsampled_y, test_size=0.3)\n",
    "up_cv_results = cross_validate(RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42), up_x_train, up_y_train, cv=5, scoring=eval_metrics, return_estimator=True)\n",
    "up_best_idx = np.argmax(up_cv_results['test_AUROC'])\n",
    "up_clf = up_cv_results['estimator'][up_best_idx]\n",
    "\n",
    "print(\"Upsampled training performance:\")\n",
    "print(\"AUROC: %f, Avg. Precision: %f, Accuracy: %f\" % (up_cv_results['test_AUROC'][up_best_idx], up_cv_results['test_avg_precision'][up_best_idx], up_cv_results['test_Accuracy'][up_best_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-blond",
   "metadata": {},
   "source": [
    "Exercise to reader: Try to evaluate the performance of the trained down and up sampled classifiers using the test sets (down_x_test, down_y_test) and (up_x_test, up_y_test). Feel free to reuse code from earlier on in the notebook, adjusting as necessary. The `evaluate_trained_clf` function will help here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-aquarium",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

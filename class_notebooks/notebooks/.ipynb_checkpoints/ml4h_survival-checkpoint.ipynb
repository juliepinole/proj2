{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning Hands On\n",
    "\n",
    "## 1. Download data of today's example (preprocessed Table):\n",
    "\n",
    "**Gene expression data for cancer samples from the TCGA database**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'TCGA-cancer-DF.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#read precalculated csv table \"TCGA-cancer-DF.zip\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df_noNA\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTCGA-cancer-DF.zip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[1;32m      6\u001b[0m df_noNA\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/cluster/courses/ml4h/jupyter/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/courses/ml4h/jupyter/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/cluster/courses/ml4h/jupyter/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/courses/ml4h/jupyter/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/cluster/courses/ml4h/jupyter/lib/python3.10/site-packages/pandas/io/common.py:794\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# ZIP Compression\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compression \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;66;03m# error: Argument 1 to \"_BytesZipFile\" has incompatible type\u001b[39;00m\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;66;03m# \"Union[str, BaseBuffer]\"; expected \"Union[Union[str, PathLike[str]],\u001b[39;00m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;66;03m# ReadBuffer[bytes], WriteBuffer[bytes]]\"\u001b[39;00m\n\u001b[0;32m--> 794\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[43m_BytesZipFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcompression_args\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m handle\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    798\u001b[0m         handles\u001b[38;5;241m.\u001b[39mappend(handle)\n",
      "File \u001b[0;32m/cluster/courses/ml4h/jupyter/lib/python3.10/site-packages/pandas/io/common.py:1037\u001b[0m, in \u001b[0;36m_BytesZipFile.__init__\u001b[0;34m(self, file, mode, archive_name, **kwargs)\u001b[0m\n\u001b[1;32m   1034\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, zipfile\u001b[38;5;241m.\u001b[39mZIP_DEFLATED)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m# error: Incompatible types in assignment (expression has type \"ZipFile\",\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m# base class \"_BufferedWriter\" defined the type as \"BytesIO\")\u001b[39;00m\n\u001b[0;32m-> 1037\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer: zipfile\u001b[38;5;241m.\u001b[39mZipFile \u001b[38;5;241m=\u001b[39m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[1;32m   1038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/zipfile.py:1251\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1251\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1253\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'TCGA-cancer-DF.zip'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#read precalculated csv table \"TCGA-cancer-DF.zip\n",
    "df_noNA=pd.read_csv(\"/cluster/courses/ml4h/data_for_users/data/TCGA-cancer-DF.zip\", index_col=0,compression=\"zip\")  \n",
    "df_noNA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create matrix X with independent variable and a dataframe with survival information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "#keep only breast cancer:\n",
    "df_noNA_reduced=df_noNA.loc[df_noNA['type']==\"BRCA\"]\n",
    "\n",
    "#keep only clinical information and some important genes https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0241924#:~:text=We%20found%20that%20the%206,prognosis%20of%20breast%20cancer%20patients :\n",
    "df_noNA_reduced=df_noNA_reduced[['CD24', 'PRRG1', 'IQSEC3', 'RCC2', 'CASP8','ERBB2','stage', 'age', 'status', 'time']]\n",
    "\n",
    "#replace Gender by 0 and 1\n",
    "dictionary = {'LIVING': 0, 'DECEASED': 1}\n",
    "df_noNA_reduced=df_noNA_reduced.replace({'status': dictionary})\n",
    "\n",
    "#replace Stage with floats\n",
    "dictionary = {'LIVING': 0, 'DECEASED': 1}\n",
    "df_noNA_reduced=df_noNA_reduced.replace({'status': dictionary})\n",
    "\n",
    "dictionary = {'Stage I': 1.5, 'Stage IA': 1.4, 'Stage IB': 1.6, 'Stage II': 2.5, 'Stage IIA': 2.4, 'Stage IIB': 2.6,\n",
    "    'Stage III': 3.5, 'Stage IIIA': 3.4, 'Stage IIIB': 3.6, 'Stage IIIC': 3.8, 'Stage IV': 4.5}\n",
    "df_noNA_reduced=df_noNA_reduced.replace({'stage': dictionary})\n",
    "    \n",
    "#remove entries with 0 time after the last follow-up: \n",
    "df_noNA_reduced=df_noNA_reduced.loc[df_noNA_reduced['time']>0]\n",
    "\n",
    "\n",
    "X = df_noNA_reduced.drop(columns=['status', 'time'])\n",
    "y = df_noNA_reduced[['status', 'time']]\n",
    "\n",
    "print(\"Total number of samples: %d \" %X.shape[0])\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines.plotting import plot_lifetimes # conda install -c conda-forge lifelines\n",
    "from lifelines import KaplanMeierFitter\n",
    "from matplotlib import pyplot as plt\n",
    "from lifelines.utils import median_survival_times\n",
    "\n",
    "\n",
    "#Visualize survival data for random 25 individuals from our dataset\n",
    "y_25=y.sample(n=25, random_state=14)\n",
    "plot_lifetimes(y_25['time'],  event_observed=y_25['status'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Visualize survival data for random all individuals using the Kaplan-Meier curve\n",
    "kmf = KaplanMeierFitter()\n",
    "kmf.fit(y['time'], event_observed=y['status'])\n",
    "\n",
    "kmf.plot_survival_function(show_censors=True, censor_styles={'ms': 7, 'marker': 'x'}, ci_show=False)\n",
    "plt.title('Survival function of TCGA breast cancer patients');\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Overall Survival\")\n",
    "plt.show()\n",
    "\n",
    "#Visualize survival data for random all individuals using the Kaplan-Meier curve - add confidence intervals \n",
    "kmf.plot_survival_function(show_censors=True, censor_styles={'ms': 7, 'marker': 'x'}, ci_show=True)\n",
    "plt.title('Survival function of TCGA breast cancer patients');\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Overall Survival\")\n",
    "plt.show()\n",
    "\n",
    "#Visualize survival data for random all individuals using the Kaplan-Meier curve - add counts of patients at risk\n",
    "kmf.plot_survival_function(show_censors=True, censor_styles={'ms': 7, 'marker': 'x'}, ci_show=True, at_risk_counts=True)\n",
    "plt.title('Survival function of TCGA breast cancer patients');\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Overall Survival\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Print median survival time, and its 95% confidence interval :\n",
    "print(\"Median survival time (estimate): %f \" %kmf.median_survival_time_)\n",
    "\n",
    "median_ci = median_survival_times(kmf.confidence_interval_)\n",
    "print(\"Median survival time (estimate) confidence interval: %f-%f \" %(median_ci['KM_estimate_lower_0.95'],median_ci['KM_estimate_upper_0.95']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plot KM curves for patients overexpressing HER2 (official gene name ERBB2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram for the ERBB2 expression:\n",
    "fig, (ax1) = plt.subplots(1, 1)\n",
    "fig.set_size_inches(8, 6)\n",
    "n, bins, patches = ax1.hist(x=X['ERBB2'], bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "ax1.grid(axis='y', alpha=0.75)\n",
    "ax1.set_xlabel('ERBB2 expression')\n",
    "ax1.set_ylabel('Frequency')\n",
    "maxfreq = n.max()\n",
    "# Set a clean upper y-axis limit.\n",
    "ax1.set_ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)\n",
    "\n",
    "\n",
    "#Let's use threshold of 4 to get patients with overexpressed ERBB2:\n",
    "exp_threshold=4\n",
    "plt.vlines(x=exp_threshold, ymin=0, ymax=maxfreq ,colors=None, linestyles='dotted')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "i1=(X['ERBB2'] <= exp_threshold)\n",
    "i2=(X['ERBB2'] > exp_threshold)\n",
    "\n",
    "#Plot KM curves for the two groups of patients:\n",
    "\n",
    "kmf = KaplanMeierFitter()\n",
    "kmf.fit(y['time'][i1], event_observed=y['status'][i1])\n",
    "kmf.plot_survival_function(show_censors=True, censor_styles={'ms': 7, 'marker': 'x'}, ci_show=True, label=\"Low ERBB2 expression\")\n",
    "\n",
    "kmf.fit(y['time'][i2], event_observed=y['status'][i2])\n",
    "kmf.plot_survival_function(show_censors=True, censor_styles={'ms': 7, 'marker': 'x'}, ci_show=True, label=\"High ERBB2 expression\")\n",
    "plt.legend(loc=\"lower left\", shadow=False, scatterpoints=1)\n",
    "\n",
    "plt.title('Survival function of TCGA breast cancer patients');\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Overall Survival\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate log-rank test for these two groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines.statistics import logrank_test\n",
    "\n",
    "results = logrank_test(y['time'][i1], y['time'][i2], event_observed_A=y['status'][i1], event_observed_B=y['status'][i2])\n",
    "results.print_summary()\n",
    "\n",
    "print(results.p_value)        # 0.4047795"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Try different splits for log-rank test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.stats.multitest as multi\n",
    "\n",
    "thresholds = (3,3.5,4,4.5,5)\n",
    "\n",
    "pvalues=np.ones(len(thresholds))\n",
    "\n",
    "for ind, threshold_value in enumerate(thresholds):    \n",
    "    i1=(X['ERBB2'] <= threshold_value)\n",
    "    i2=(X['ERBB2'] > threshold_value)\n",
    "\n",
    "    #Plot KM curves for the two groups of patients:\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(y['time'][i1], event_observed=y['status'][i1])\n",
    "    kmf.plot_survival_function(show_censors=True, censor_styles={'ms': 7, 'marker': 'x'}, ci_show=True, label=\"Low ERBB2 expression\")\n",
    "    kmf.fit(y['time'][i2], event_observed=y['status'][i2])\n",
    "    kmf.plot_survival_function(show_censors=True, censor_styles={'ms': 7, 'marker': 'x'}, ci_show=True, label=\"High ERBB2 expression\")\n",
    "    plt.legend(loc=\"lower left\", shadow=False, scatterpoints=1)\n",
    "    plt.title('Survival function of TCGA breast cancer patients');\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel(\"Overall Survival\")\n",
    "    plt.show()\n",
    "    results = logrank_test(y['time'][i1], y['time'][i2], event_observed_A=y['status'][i1], event_observed_B=y['status'][i2])\n",
    "    print(\"p-value for threshold = %f : %f\" % (threshold_value,results.p_value))\n",
    "    pvalues[ind]=results.p_value\n",
    "\n",
    "print(\"\\nInitial p-values:\")\n",
    "print(pvalues)\n",
    "print(\"\\nAdjusted p-values:\")\n",
    "_, pvals_adj, _, _ = multi.multipletests(pvalues, alpha=0.05, method='fdr_bh')\n",
    "print(pvals_adj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plot smoothed hasard function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import NelsonAalenFitter\n",
    "naf = NelsonAalenFitter()\n",
    "\n",
    "bandwidth = 1000\n",
    "\n",
    "exp_threshold=4\n",
    "i1=(X['ERBB2'] <= exp_threshold)\n",
    "i2=(X['ERBB2'] > exp_threshold)\n",
    "\n",
    "\n",
    "naf.fit(y['time'][i1], event_observed=y['status'][i1], label=\"Low ERBB2 expression\")\n",
    "ax = naf.plot_hazard(bandwidth=bandwidth)\n",
    "\n",
    "naf.fit(y['time'][i2], event_observed=y['status'][i2], label=\"High ERBB2 expression\")\n",
    "naf.plot_hazard(ax=ax, bandwidth=bandwidth)\n",
    "\n",
    "plt.title(\"Hazard function depending on the ERBB2 expression | bandwidth=%.1f\" % bandwidth);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Cox proportional hazard model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#stardartize the data (so that coefficients make more sense):\n",
    "scaled_X = X.copy()\n",
    "col_names = X.columns\n",
    "features = scaled_X[col_names]\n",
    "scaler = StandardScaler().fit(features.values)\n",
    "features = scaler.transform(features.values)\n",
    "scaled_X[col_names] = features\n",
    "\n",
    "scaled_data=pd.concat([scaled_X, y], axis=1)\n",
    "\n",
    "cph=CoxPHFitter()\n",
    "cph.fit(scaled_data, \"time\", event_col=\"status\")\n",
    "cph.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Plot log(hasard ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Build Cox regression on train and validate on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_data.drop(columns=['time','status']), scaled_data[['time','status']], test_size=0.3, shuffle=True, random_state=42) \n",
    "\n",
    "#Train CoxPH model on the 70% of the data\n",
    "print(\"Training CoxPH model on the train set (70% of data)\")\n",
    "cph=CoxPHFitter()\n",
    "cph.fit(pd.concat([X_train, y_train], axis=1), \"time\", event_col=\"status\")\n",
    "cph.print_summary()\n",
    "\n",
    "#Validate on the test set (30% of the data)\n",
    "expectedEventTime=cph.predict_expectation(pd.concat([X_test, y_test], axis=1))\n",
    "\n",
    "\n",
    "# Print the c-index for the test set:\n",
    "print('\\nConcordance index on the test set (30%% of the data): %.2f' % cph.score(pd.concat([X_test, y_test], axis=1), scoring_method ='concordance_index'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Build Cox regression with clinical data only on train and validate on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_data[['stage', 'age']], scaled_data[['time','status']], test_size=0.3, shuffle=True, random_state=42) \n",
    "\n",
    "#Train CoxPH model on the 70% of the data\n",
    "print(\"Training CoxPH model on the train set (70% of data)\")\n",
    "cph=CoxPHFitter()\n",
    "cph.fit(pd.concat([X_train, y_train], axis=1), \"time\", event_col=\"status\")\n",
    "cph.print_summary()\n",
    "\n",
    "#Validate on the test set (30% of the data)\n",
    "expectedEventTime=cph.predict_expectation(pd.concat([X_test, y_test], axis=1))\n",
    "\n",
    "\n",
    "# Print the c-index for the test set:\n",
    "print('\\nConcordance index on the test set (30%% of the data): %.2f' % cph.score(pd.concat([X_test, y_test], axis=1), scoring_method ='concordance_index'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Build Cox regression excluding clinical data on train and validate on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_data.drop(columns=['time','status','stage', 'age']), scaled_data[['time','status']], test_size=0.3, shuffle=True, random_state=42) \n",
    "\n",
    "#Train CoxPH model on the 70% of the data\n",
    "print(\"Training CoxPH model on the train set (70% of data)\")\n",
    "cph=CoxPHFitter()\n",
    "cph.fit(pd.concat([X_train, y_train], axis=1), \"time\", event_col=\"status\")\n",
    "cph.print_summary()\n",
    "\n",
    "#Validate on the test set (30% of the data)\n",
    "expectedEventTime=cph.predict_expectation(pd.concat([X_test, y_test], axis=1))\n",
    "\n",
    "\n",
    "# Print the c-index for the test set:\n",
    "print('\\nConcordance index on the test set (30%% of the data): %.2f' % cph.score(pd.concat([X_test, y_test], axis=1), scoring_method ='concordance_index'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Build LASSO Cox regression on train and validate on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_data.drop(columns=['time','status']), scaled_data[['time','status']], test_size=0.3, shuffle=True, random_state=42) \n",
    "\n",
    "#Train LASSO CoxPH model on the 70% of the data\n",
    "print(\"Training CoxPH model on the train set (70% of data)\")\n",
    "\n",
    "myLambda=0.04 #in theory, you can find the best values of Lambda using Cross-Validation (check the lecture and Jupiter NB on Regression)\n",
    "\n",
    "print('Will use Lambda = %.2f' % myLambda)\n",
    "\n",
    "cph=CoxPHFitter(penalizer=myLambda, l1_ratio = 1) # set l1_ratio = 0 to do Ridge regression, or a value between 0 and 1 for the Elastic Net\n",
    "cph.fit(pd.concat([X_train, y_train], axis=1), \"time\", event_col=\"status\")\n",
    "cph.print_summary()\n",
    "\n",
    "#Validate on the test set (30% of the data)\n",
    "expectedEventTime=cph.predict_expectation(pd.concat([X_test, y_test], axis=1))\n",
    "\n",
    "\n",
    "# Print the c-index for the test set:\n",
    "print('\\nConcordance index on the test set (30%% of the data): %.2f' % cph.score(pd.concat([X_test, y_test], axis=1), scoring_method ='concordance_index'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Run Random Survival Forest on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sksurv.datasets import load_gbsg2 #conda install -c sebp scikit-survival\n",
    "from sksurv.preprocessing import OneHotEncoder\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "\n",
    "y=scaled_data[['status','time']]\n",
    "y[\"status\"] = y[\"status\"].astype(bool)\n",
    "y=y.to_records(index=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_data.drop(columns=['status','time']), y, test_size=0.3, shuffle=True, random_state=42) \n",
    "\n",
    "#Train Random Forest model on the 70% of the data\n",
    "print(\"Training Random Forest model on the train set (70% of data)\")\n",
    "\n",
    "nTrees=40 #in theory, you can find the best values of Lambda using Cross-Validation (check the lecture and Jupiter NB on Regression)\n",
    "my_max_depth=2  #in theory, you can find the best values of tree depth using Cross-Validation (check the lecture and Jupiter NB on Regression)\n",
    "\n",
    "print('Will use %d trees' % nTrees)\n",
    "print('Will use maximal depth = %d' % my_max_depth)\n",
    "\n",
    "rsf = RandomSurvivalForest(n_estimators=nTrees,\n",
    "                           max_depth = my_max_depth,                          \n",
    "                           n_jobs=-1,\n",
    "                           random_state=42)\n",
    "\n",
    "rsf.fit(X_train, y_train)\n",
    "\n",
    "print('Concordance index on the training set: %.2f' % rsf.score(X_train, y_train))\n",
    "\n",
    "print('Concordance index on the test set:%.2f' % rsf.score(X_test, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
